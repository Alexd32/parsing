import json
import asyncio
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from fetcher import Fetcher
from parser import parse_item
from error_handler import ErrorHandler
import os


def load_config(path):
    with open(path, encoding="utf-8") as f:
        return json.load(f)


def load_proxies(path):
    try:
        with open(path, encoding="utf-8") as f:
            return [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        return []


def build_url(base_url: str, page: int) -> str:
    """–°—Ç—Ä–æ–∏–º URL –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞"""
    if page == 1:
        return base_url.replace("/page/{page}/", "") + "/"
    return base_url.format(page=page)


async def run():
    cfg = load_config("configs/brushme.json")
    proxies = load_proxies("proxies.txt") if cfg["technologies"].get("proxy", False) else []

    # —Ä–∞–∑–±–æ—Ä rate_limit: "1/3s" –∏–ª–∏ "3s"
    rl = cfg["fetch"]["rate_limit"]
    if "/" in rl:
        _, value = rl.split("/")
        rate_limit = float(value.replace("s", ""))
    else:
        rate_limit = float(rl.replace("s", ""))

    headers = cfg["fetch"].get("headers")
    fetcher = Fetcher(proxies, headers, rate_limit)
    error_handler = ErrorHandler(retries=3, backoff=3)

    list_cfg = cfg["selectors"]["list_page"]
    item_fields = cfg["selectors"]["item_page"]["fields"]
    base_url = cfg.get("base_url") or f"https://{cfg['domain']}"

    out_file_path = "results.txt"
    first_write = not os.path.exists(out_file_path)
    header_fields = list(item_fields.keys()) + ["url", "timestamp"]

    with open(out_file_path, "a", encoding="utf-8") as out_file:
        if first_write:
            out_file.write(";".join(header_fields) + "\n")

        # –æ–±—Ö–æ–¥–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ –ø—É—Å—Ç–æ–π
        for page in range(1, 500):
            url = build_url(list_cfg["url"], page)
            print(f"üìÑ –ö–∞—Ç–∞–ª–æ–≥: {url}")

            html = await error_handler.handle(fetcher.get, url)
            if not html:
                print(f"‚õ≥ –°—Ç–æ–ø: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {url}")
                break

            soup = BeautifulSoup(html, "lxml")
            nodes = soup.select("a.woocommerce-LoopProduct-link")
            if not nodes:
                if page == 1:
                    print("‚ùå –ù–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ—Ç —Ç–æ–≤–∞—Ä–æ–≤.")
                else:
                    print("‚õ≥ –ü—É—Å—Ç–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞, –∫–æ–Ω–µ—Ü –∫–∞—Ç–∞–ª–æ–≥–∞.")
                break

            links = []
            for a in nodes:
                href = a.get("href")
                if href:
                    full = href if href.startswith("http") else urljoin(base_url + "/", href)
                    links.append(full)

            print(f"  –Ω–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫")

            for link in links:
                html_item = await error_handler.handle(fetcher.get, link)
                if not html_item:
                    print(f"    ‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –∫–∞—Ä—Ç–æ—á–∫–∏ {link}")
                    continue
                try:
                    item = parse_item(html_item, item_fields, link)
                    line = ";".join((item.get(field, "") or "").replace("\n", " ").strip() for field in header_fields)
                    out_file.write(line + "\n")
                    out_file.flush()
                    print(f"    ‚úÖ {item.get('title') or link}")
                except Exception as e:
                    print(f"    ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {link}: {e}")


if __name__ == "__main__":
    asyncio.run(run())
