import json
import asyncio
import os
import re
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from parser import parse_item
from error_handler import ErrorHandler
from selenium_fetcher import SeleniumFetcher  # <-- –∏—Å–ø–æ–ª—å–∑—É–µ–º Selenium

# üîß –í—ã–±–æ—Ä –∫–æ–Ω—Ñ–∏–≥–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π –Ω—É–∂–Ω—ã–π)
# CONFIG_PATH = "configs/brushme.json"
CONFIG_PATH = "configs/apteka.json"


def load_config(path: str) -> dict:
    with open(path, encoding="utf-8") as f:
        return json.load(f)


def slugify(name: str) -> str:
    return re.sub(r"[^a-z0-9]+", "_", (name or "site").lower()).strip("_")


def parse_selector_and_attr(selector: str):
    if "::attr" in selector:
        css, attr = selector.split("::attr", 1)
        return css.strip(), attr.strip("() ").strip()
    return selector.strip(), None


def build_url(template: str, page: int) -> str:
    if "{page}" not in template:
        return template

    if page == 1:
        url = template
        url = url.replace("/page/{page}/", "/")
        url = url.replace("page-{page}/", "")
        url = url.replace("?page={page}", "")
        url = url.replace("&page={page}", "")
        url = url.replace("{page}", "")
        url = url.replace("://", "¬ß¬ß")
        while "//" in url:
            url = url.replace("//", "/")
        return url.replace("¬ß¬ß", "://")
    else:
        return template.replace("{page}", str(page))


async def run():
    cfg = load_config(CONFIG_PATH)

    # rate_limit (–Ω–∞–ø—Ä–∏–º–µ—Ä "1/2s" –∏–ª–∏ "3s")
    rl = cfg["fetch"]["rate_limit"]
    if "/" in rl:
        _, value = rl.split("/")
        rate_limit = float(value.replace("s", ""))
    else:
        rate_limit = float(rl.replace("s", ""))

    headers = cfg["fetch"].get("headers", {})
    user_agent = headers.get("User-Agent")

    # SeleniumFetcher –∫–∞–∫ –µ–¥–∏–Ω–∞—è —Å–µ—Å—Å–∏—è –±—Ä–∞—É–∑–µ—Ä–∞
    fetcher = SeleniumFetcher(
        rate_limit=rate_limit,
        user_agent=user_agent,
        headless=True,  # –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –≤—ã–∫–ª—é—á–∏—Ç—å
        accept_language=headers.get("Accept-Language", "ru-RU,ru;q=0.9,en;q=0.8"),
        page_load_timeout=60,
    )
    error_handler = ErrorHandler(retries=2, backoff=4)  # retries –ø–æ–º–µ–Ω—å—à–µ: –±—Ä–∞—É–∑–µ—Ä ¬´—Ç—è–∂—ë–ª—ã–π¬ª

    list_cfg = cfg["selectors"]["list_page"]
    item_fields = cfg["selectors"]["item_page"]["fields"]
    base_url = (cfg.get("base_url") or f"https://{cfg['domain']}").rstrip("/") + "/"

    out_file_path = f"results_{slugify(cfg.get('name', 'site'))}.txt"
    first_write = not os.path.exists(out_file_path)
    header_fields = list(item_fields.keys()) + ["url", "timestamp"]

    css_item_link, attr_item_link = parse_selector_and_attr(list_cfg["item_link"])

    # CSS, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ª–∂–µ–Ω –ø–æ—è–≤–∏—Ç—å—Å—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∫–∞—Ç–∞–ª–æ–≥–∞ (—á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, —á—Ç–æ —Ç–æ–≤–∞—Ä—ã –ø—Ä–æ–≥—Ä—É–∑–∏–ª–∏—Å—å)
    catalog_wait_css = list_cfg.get("wait_css") or css_item_link

    try:
        with open(out_file_path, "a", encoding="utf-8") as out_file:
            if first_write:
                out_file.write(";".join(header_fields) + "\n")

            for page in range(1, 500):
                url = build_url(list_cfg["url"], page)
                print(f"üìÑ –ö–∞—Ç–∞–ª–æ–≥: {url}")

                # –∂–¥—ë–º –ø–æ—è–≤–ª–µ–Ω–∏—è –∫–∞—Ä—Ç–æ—á–µ–∫ –ø–æ CSS
                html = await error_handler.handle(
                    lambda u: fetcher.get(u, wait_css=catalog_wait_css, timeout=40),
                    url
                )
                if not html:
                    print(f"‚õ≥ –°—Ç–æ–ø: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {url}")
                    break

                soup = BeautifulSoup(html, "lxml")
                nodes = soup.select(css_item_link)
                if not nodes:
                    if page == 1:
                        print("‚ùå –ù–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ—Ç —Ç–æ–≤–∞—Ä–æ–≤ (–ø—Ä–æ–≤–µ—Ä—å item_link/wait_css).")
                    else:
                        print("‚õ≥ –ü—É—Å—Ç–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞, –∫–æ–Ω–µ—Ü –∫–∞—Ç–∞–ª–æ–≥–∞.")
                    break

                links = []
                for node in nodes:
                    href = node.get(attr_item_link) if attr_item_link else node.get("href")
                    if href:
                        links.append(urljoin(base_url, href))

                print(f"  –Ω–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫")

                for link in links:
                    html_item = await error_handler.handle(
                        lambda u: fetcher.get(u, wait_css=None, timeout=40),
                        link
                    )
                    if not html_item:
                        print(f"    ‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –∫–∞—Ä—Ç–æ—á–∫–∏ {link}")
                        continue
                    try:
                        item = parse_item(html_item, item_fields, link)
                        line = ";".join((item.get(field, "") or "").replace("\n", " ").strip() for field in header_fields)
                        out_file.write(line + "\n")
                        out_file.flush()
                        print(f"    ‚úÖ {item.get('title') or link}")
                    except Exception as e:
                        print(f"    ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {link}: {e}")
    finally:
        await fetcher.close()


if __name__ == "__main__":
    asyncio.run(run())
